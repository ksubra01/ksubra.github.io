{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The main objective of this project is to detect the absence or presence of facemask using the OAK-D camera feed of turtlebot by deploying Neural Networks. If the turtlebot camera detects a face without mask, it displays the output image as 'without mask' and move towards the detected face. Training data and Neural network for mask detection is custom made, MTCNN (inbuilt algorithm) is being used for face coordinates. An insight of Project Goals and procedure that we have followed and results obtained are briefly discussed here Team Members: Karthick Subramanian Rohan Khare Praveen Paidi a. Updates/Refreshes We made our own Neural Network for the feature generation by taking training data of images from OAK-D camera. From the custom NN model, We deployed model to ROS node which does face mask recognition in real time by subscribing to real time data current_frame. We used inbuilt software called MTCNN face detection algorithm for creation of the bounding box for faces which is also subscribing to real time data current_frame. We published a custom topic /go/image from face mask recognition ROS node to MTCNN_vel_pub(fcae tracking) node which is subscriber to /go/image and publishes /cmd/vel. If the MTCNN_vel_det(face tracking) is being sent face without mask, it publishes velocity to move towards that face by using Face bounding box coordinates. We tried mapping classroom environment, but due to constraints we sticked to main scope of project which is NN deployment. We aim to improve by performing dynamic obstacle avoidance (SLAM). b. Project Process Although we were able to deploy custom model to ROS node in the previous week, Due to accuracy and some other constraints we revised the schedule from the last week and performed required tasks such as data collection and real time deployment with iterations. Following is the Process chart : The complete work flow of our project as follows: c. Work Flow Initially we collected dataset consisting of huge variations i.e Face with mask and Without Mask at different distances from OAK-D camera at different backgrounds with multiple gestures being performed. Few Instances of preliminary dataset With Face Mask - If the frame has a face with mask. - Class 0 Without Face Mask - If the frame has a face without mask and some ambiguity cases of half frames and fully blurred faces. - Class 1 No recognizable face in Frame - If the frame has no recognizable face and some distant images which are very unclear. - Class 2 Custom Neural Network for Face mask Detection: Then with the custom Neural Network we devloped, we generated feature maps which shows as following: Even in this zoomed in images, the amount of pixels covering pixels are very few and very difficult to classify the images based on mask from the limited data with high number of variations. Although training Accuracy is good by optimizing the algorithm and increasing number of epochs, testing accuracy is not good with only 65% . If we perform real time detection with this, it would be highly inaccurate. Counter Measures We updated the Training data with Face with and without mask at limited distance at proper light source with few gestures. Few Instances of preliminary dataset With Face Mask - If the frame has face with mask Without Face Mask - If the frame has consisting of face without mask and some ambiguity cases of half frames and fully blurred faces. No recognizable face in Frame - If the frame has no recognizable face and some distant images which are very unclear. we split the data into Training and Testing in 90% and 10% and Training accuracy as follows: Training data Classification: Testing data Classification: After Training on data, we got adequate accuracy in Training along with a good amount of testing accuracy of 89% Training Accuracy: With 89% testing accuracy, we tested on few absolute new images as well before deploying to ROS node for real time detection. It is predicting good, doesn't take much time as well. We transferred the model_infernce to ROS node using tensorflow. d. ROS Network With model being imported, we created total of 3 nodes and 1 custom topic to complete actions. Mask Detector node is a subscriber to /color/image and publishes custom topic /go/image, displays output windows of current frame and detection frame. face Detector node is a subscriber to /color/image and displays output window of bounding box. face tracking node is a subscriber to /go/image and publishes /cmd/vel. Complete ROS architecture rqt_gul_py for the generation of ROS graph is also shown in the graph. e. Tradeoffs Main Challenges Training time As the Neural Network model is heavy, we trained set of images in GPU and obtained the model. Then transferred the model weights to ROS node to take one frame at a time.This has no lag and almost accurate. Movement of Turtlebot For the Face detection and creating a bounding box, it takes more time in real time.As a result, output window with bounding box has a lag of few seconds. Which inturn carries the lag in moving the turtlebot towards the bounding box containing face without the mask. Detection vs Movement Although Face mask detection takes place at real time, turtlebot is moving after few moments later the detection and viceversa. Tradeoffs due to constraints Fitting Training data We used Adam algorithm to save the training time. We chose number of epochs to 5 to not overfit on training data by considering bias and variance trade off. If we furthur increase the number of epochs, we may lose the testing accuracy. Non Generalized Detection As the dataset is limited, detection using OAK-D camera at different environment with different person is not feasible. To detect, proper lighting and limited distance to the camera are required. With limited training data, we are able to test at few places with absolute new faces successfully. Bounding box trade off We used face detection algorithm along with velocity subscriber not with face mask detector to increase the speed of detection. It significantly increased the speed of detection by almost 2 to 3 sec at the cost of movement of bot. In overall process, we gave stress to reduce type 2 error (False Negative) . 1. As it is a face mask detection, a person without mask should be detected properly. 2. If the bot has ambiguous case like image is blur or face is not complete in the frame, it is mostly detected as face without mask. 3. If the face is mostly out of the frame, it is being detected as no face recognized. 4. We trained the data accordingly to reduce the false negatives due to which there may be few errors during the transition of frames such as face entering or exiting the frame. f. Videos of Working Model Video 1- TURTLEBOT VIEW OF TESTING Real time video of turtlebot view detecting face without mask : Screen recording of mask is being pulled down and not covering the face. We have not trained this ambiguous cases earlier, still it is successfully detecting the classification Video 2- AMBIGUOUS CASE TESTING WITH NO TRAINING Real time video of turtlebot detecting face without mask : * In this video, turtlebot movement of simultaneous ROS nodes in video1. Video 3 - ABSOLUTE UNKNOWN FACE TESTING RealTime detection of Presence or Ansence of mask with absolute zero training of Person's face * In this video, Face present in the frame is not at all used in training data. It is still successfully able to detect the classification accurately and moving towards the face without mask. g. Goals, Process and Results s An insight of Project Goals and procedure that we have followed and results obtained are briefly discussed in this:","title":"Home"},{"location":"#introduction","text":"The main objective of this project is to detect the absence or presence of facemask using the OAK-D camera feed of turtlebot by deploying Neural Networks. If the turtlebot camera detects a face without mask, it displays the output image as 'without mask' and move towards the detected face. Training data and Neural network for mask detection is custom made, MTCNN (inbuilt algorithm) is being used for face coordinates. An insight of Project Goals and procedure that we have followed and results obtained are briefly discussed here Team Members: Karthick Subramanian Rohan Khare Praveen Paidi","title":"Introduction"},{"location":"#a-updatesrefreshes","text":"We made our own Neural Network for the feature generation by taking training data of images from OAK-D camera. From the custom NN model, We deployed model to ROS node which does face mask recognition in real time by subscribing to real time data current_frame. We used inbuilt software called MTCNN face detection algorithm for creation of the bounding box for faces which is also subscribing to real time data current_frame. We published a custom topic /go/image from face mask recognition ROS node to MTCNN_vel_pub(fcae tracking) node which is subscriber to /go/image and publishes /cmd/vel. If the MTCNN_vel_det(face tracking) is being sent face without mask, it publishes velocity to move towards that face by using Face bounding box coordinates. We tried mapping classroom environment, but due to constraints we sticked to main scope of project which is NN deployment. We aim to improve by performing dynamic obstacle avoidance (SLAM).","title":"a. Updates/Refreshes"},{"location":"#b-project-process","text":"Although we were able to deploy custom model to ROS node in the previous week, Due to accuracy and some other constraints we revised the schedule from the last week and performed required tasks such as data collection and real time deployment with iterations. Following is the Process chart : The complete work flow of our project as follows:","title":"b. Project Process"},{"location":"#c-work-flow","text":"Initially we collected dataset consisting of huge variations i.e Face with mask and Without Mask at different distances from OAK-D camera at different backgrounds with multiple gestures being performed. Few Instances of preliminary dataset With Face Mask - If the frame has a face with mask. - Class 0 Without Face Mask - If the frame has a face without mask and some ambiguity cases of half frames and fully blurred faces. - Class 1 No recognizable face in Frame - If the frame has no recognizable face and some distant images which are very unclear. - Class 2 Custom Neural Network for Face mask Detection: Then with the custom Neural Network we devloped, we generated feature maps which shows as following: Even in this zoomed in images, the amount of pixels covering pixels are very few and very difficult to classify the images based on mask from the limited data with high number of variations. Although training Accuracy is good by optimizing the algorithm and increasing number of epochs, testing accuracy is not good with only 65% . If we perform real time detection with this, it would be highly inaccurate. Counter Measures We updated the Training data with Face with and without mask at limited distance at proper light source with few gestures. Few Instances of preliminary dataset With Face Mask - If the frame has face with mask Without Face Mask - If the frame has consisting of face without mask and some ambiguity cases of half frames and fully blurred faces. No recognizable face in Frame - If the frame has no recognizable face and some distant images which are very unclear. we split the data into Training and Testing in 90% and 10% and Training accuracy as follows: Training data Classification: Testing data Classification: After Training on data, we got adequate accuracy in Training along with a good amount of testing accuracy of 89% Training Accuracy: With 89% testing accuracy, we tested on few absolute new images as well before deploying to ROS node for real time detection. It is predicting good, doesn't take much time as well. We transferred the model_infernce to ROS node using tensorflow.","title":"c. Work Flow"},{"location":"#d-ros-network","text":"With model being imported, we created total of 3 nodes and 1 custom topic to complete actions. Mask Detector node is a subscriber to /color/image and publishes custom topic /go/image, displays output windows of current frame and detection frame. face Detector node is a subscriber to /color/image and displays output window of bounding box. face tracking node is a subscriber to /go/image and publishes /cmd/vel. Complete ROS architecture rqt_gul_py for the generation of ROS graph is also shown in the graph.","title":"d. ROS Network"},{"location":"#e-tradeoffs","text":"Main Challenges Training time As the Neural Network model is heavy, we trained set of images in GPU and obtained the model. Then transferred the model weights to ROS node to take one frame at a time.This has no lag and almost accurate. Movement of Turtlebot For the Face detection and creating a bounding box, it takes more time in real time.As a result, output window with bounding box has a lag of few seconds. Which inturn carries the lag in moving the turtlebot towards the bounding box containing face without the mask. Detection vs Movement Although Face mask detection takes place at real time, turtlebot is moving after few moments later the detection and viceversa. Tradeoffs due to constraints Fitting Training data We used Adam algorithm to save the training time. We chose number of epochs to 5 to not overfit on training data by considering bias and variance trade off. If we furthur increase the number of epochs, we may lose the testing accuracy. Non Generalized Detection As the dataset is limited, detection using OAK-D camera at different environment with different person is not feasible. To detect, proper lighting and limited distance to the camera are required. With limited training data, we are able to test at few places with absolute new faces successfully. Bounding box trade off We used face detection algorithm along with velocity subscriber not with face mask detector to increase the speed of detection. It significantly increased the speed of detection by almost 2 to 3 sec at the cost of movement of bot. In overall process, we gave stress to reduce type 2 error (False Negative) . 1. As it is a face mask detection, a person without mask should be detected properly. 2. If the bot has ambiguous case like image is blur or face is not complete in the frame, it is mostly detected as face without mask. 3. If the face is mostly out of the frame, it is being detected as no face recognized. 4. We trained the data accordingly to reduce the false negatives due to which there may be few errors during the transition of frames such as face entering or exiting the frame.","title":"e. Tradeoffs"},{"location":"#f-videos-of-working-model","text":"Video 1- TURTLEBOT VIEW OF TESTING Real time video of turtlebot view detecting face without mask : Screen recording of mask is being pulled down and not covering the face. We have not trained this ambiguous cases earlier, still it is successfully detecting the classification Video 2- AMBIGUOUS CASE TESTING WITH NO TRAINING Real time video of turtlebot detecting face without mask : * In this video, turtlebot movement of simultaneous ROS nodes in video1. Video 3 - ABSOLUTE UNKNOWN FACE TESTING RealTime detection of Presence or Ansence of mask with absolute zero training of Person's face * In this video, Face present in the frame is not at all used in training data. It is still successfully able to detect the classification accurately and moving towards the face without mask.","title":"f. Videos of Working Model"},{"location":"#g-goals-process-and-results-s","text":"An insight of Project Goals and procedure that we have followed and results obtained are briefly discussed in this:","title":"g. Goals, Process and Results s"},{"location":"Face_Bounding_box/","text":"Introduction Real time ROS node for Bounding box of face Requirements # Bounding box detection of face. # Code import rclpy from rclpy.node import Node from sensor_msgs.msg import Image from cv_bridge import CvBridge import cv2 from mtcnn import MTCNN node #Step 1 Data class FaceDetectorNode(Node): def __init__(self): super().__init__('face_detector_node') self.bridge = CvBridge() # Subscribe to the camera topic self.subscription = self.create_subscription(Image,'/color/image',self.image_callback,10) # Initialize the MTCNN detector self.detector = MTCNN() def image_callback(self, msg): # Convert ROS image message to OpenCV image image = self.bridge.imgmsg_to_cv2(msg) # Detect faces in the image faces = self.detector.detect_faces(image) # Draw bounding boxes around the detected faces for face in faces: x, y, width, height = face['box'] cv2.rectangle(image, (x, y), (x+width, y+height), (0, 255, 0), 2) # Display the output image cv2.imshow('Output', image) cv2.waitKey(1) calling the node def main(args=None): rclpy.init(args=args) node = FaceDetectorNode() rclpy.spin(node) node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()","title":"Face Bounding Box"},{"location":"Face_Bounding_box/#introduction","text":"","title":"Introduction"},{"location":"Face_Bounding_box/#real-time-ros-node-for-bounding-box-of-face","text":"","title":"Real time ROS node for Bounding box of face"},{"location":"Face_Bounding_box/#requirements","text":"# Bounding box detection of face. # Code import rclpy from rclpy.node import Node from sensor_msgs.msg import Image from cv_bridge import CvBridge import cv2 from mtcnn import MTCNN","title":"Requirements"},{"location":"Face_Bounding_box/#node","text":"#Step 1 Data class FaceDetectorNode(Node): def __init__(self): super().__init__('face_detector_node') self.bridge = CvBridge() # Subscribe to the camera topic self.subscription = self.create_subscription(Image,'/color/image',self.image_callback,10) # Initialize the MTCNN detector self.detector = MTCNN() def image_callback(self, msg): # Convert ROS image message to OpenCV image image = self.bridge.imgmsg_to_cv2(msg) # Detect faces in the image faces = self.detector.detect_faces(image) # Draw bounding boxes around the detected faces for face in faces: x, y, width, height = face['box'] cv2.rectangle(image, (x, y), (x+width, y+height), (0, 255, 0), 2) # Display the output image cv2.imshow('Output', image) cv2.waitKey(1)","title":"node"},{"location":"Face_Bounding_box/#calling-the-node","text":"def main(args=None): rclpy.init(args=args) node = FaceDetectorNode() rclpy.spin(node) node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()","title":"calling the node"},{"location":"Listing/","text":"Source code and dependencies Code repository All codes, Codes for NN and nodes NN Code File, NN code Mask Detector, Mask detector Face Bounding Box, Bounding box Velocity Publisher, velocity publisher Dependencies Programming Language: Python 3.8.10 Libraries: Numpy: 1.23.5 Scipy: 1.10.1 Matplotlib: 3.1.2 rclpy: 1.9.3 tensorflow: 2.12.0 ros2: galactic MTCNN: 0.1.1 cv2 Bridge: 3.1.3 Installations pip install numpy==1.23.5 pip install scipy==1.10.1 pip install matplotlib==3.1.2 pip install rclpy==1.9.3 pip install tensorflow==2.12.0 pip install MTCNN==0.1.1 pip install opencv-python-headless==3.1.3.4 DATSET Dataset and model, Complete Folder Size of the Dataset- 69 Mb Dataset, Dataset Although you can get model after running the code on provided dataset, we are providing just for reference. Model, Model","title":"Code and Dependencies"},{"location":"Listing/#source-code-and-dependencies","text":"","title":"Source code and dependencies"},{"location":"Listing/#code-repository","text":"All codes, Codes for NN and nodes NN Code File, NN code Mask Detector, Mask detector Face Bounding Box, Bounding box Velocity Publisher, velocity publisher","title":"Code repository"},{"location":"Listing/#dependencies","text":"Programming Language: Python 3.8.10 Libraries: Numpy: 1.23.5 Scipy: 1.10.1 Matplotlib: 3.1.2 rclpy: 1.9.3 tensorflow: 2.12.0 ros2: galactic MTCNN: 0.1.1 cv2 Bridge: 3.1.3","title":"Dependencies"},{"location":"Listing/#installations","text":"pip install numpy==1.23.5 pip install scipy==1.10.1 pip install matplotlib==3.1.2 pip install rclpy==1.9.3 pip install tensorflow==2.12.0 pip install MTCNN==0.1.1 pip install opencv-python-headless==3.1.3.4","title":"Installations"},{"location":"Listing/#datset","text":"Dataset and model, Complete Folder Size of the Dataset- 69 Mb Dataset, Dataset Although you can get model after running the code on provided dataset, we are providing just for reference. Model, Model","title":"DATSET"},{"location":"Mask_Detector/","text":"Introduction Real time ROS node for Face Mask Detection Model Requirements # RealTime for Face detection. # Code import sys import cv2 import time import rclpy import numpy as np from rclpy.node import Node from std_msgs.msg import String from sensor_msgs.msg import Image from geometry_msgs.msg import Point from cv_bridge import CvBridge, CvBridgeError from tensorflow.keras.models import load_model from geometry_msgs.msg import Twist node #Step 1 Data class MaskDetector(Node): def __init__(self): super().__init__('subscriber') self.br = CvBridge() print(\"Subscribed to the video feed>>>>\") self.subscription = self.create_subscription(Image,'/color/image', self.listener_callback, 10) self.model = load_model(r'/home/ksubra25/Downloads/model_for_inference.h5') #### Publisher #### self.publisher = self.create_publisher(Image,'/go/image', 10) self.vel = Twist() def listener_callback(self,data): current_frame = self.br.imgmsg_to_cv2(data) send_frame = current_frame current_frame=cv2.resize(current_frame,(300,300)) current_frame=cv2.cvtColor(current_frame,cv2.COLOR_BGR2RGB) self.new_frame = current_frame current_frame= current_frame/255.0 cv2.imshow(\"image\",current_frame) current_frame= np.expand_dims(current_frame,axis=0) predictions=self.model.predict(current_frame) class_idx=np.argmax(predictions) if class_idx==0: label='with mask' elif class_idx==1: label='Without mask' self.publisher.publish(self.br.cv2_to_imgmsg(send_frame)) else: label='no face detected' cv2.putText(self.new_frame,label,(10,30),cv2.FONT_HERSHEY_SIMPLEX,1.0,(0,255,0),2) cv2.imshow('Detection',self.new_frame) cv2.waitKey(1) calling the node def main(args=None): rclpy.init(args=args) image_subscriber = MaskDetector() rclpy.spin(image_subscriber) image_subscriber.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()","title":"Mask Detector"},{"location":"Mask_Detector/#introduction","text":"","title":"Introduction"},{"location":"Mask_Detector/#real-time-ros-node-for-face-mask-detection-model","text":"","title":"Real time ROS node for Face Mask Detection Model"},{"location":"Mask_Detector/#requirements","text":"# RealTime for Face detection. # Code import sys import cv2 import time import rclpy import numpy as np from rclpy.node import Node from std_msgs.msg import String from sensor_msgs.msg import Image from geometry_msgs.msg import Point from cv_bridge import CvBridge, CvBridgeError from tensorflow.keras.models import load_model from geometry_msgs.msg import Twist","title":"Requirements"},{"location":"Mask_Detector/#node","text":"#Step 1 Data class MaskDetector(Node): def __init__(self): super().__init__('subscriber') self.br = CvBridge() print(\"Subscribed to the video feed>>>>\") self.subscription = self.create_subscription(Image,'/color/image', self.listener_callback, 10) self.model = load_model(r'/home/ksubra25/Downloads/model_for_inference.h5') #### Publisher #### self.publisher = self.create_publisher(Image,'/go/image', 10) self.vel = Twist() def listener_callback(self,data): current_frame = self.br.imgmsg_to_cv2(data) send_frame = current_frame current_frame=cv2.resize(current_frame,(300,300)) current_frame=cv2.cvtColor(current_frame,cv2.COLOR_BGR2RGB) self.new_frame = current_frame current_frame= current_frame/255.0 cv2.imshow(\"image\",current_frame) current_frame= np.expand_dims(current_frame,axis=0) predictions=self.model.predict(current_frame) class_idx=np.argmax(predictions) if class_idx==0: label='with mask' elif class_idx==1: label='Without mask' self.publisher.publish(self.br.cv2_to_imgmsg(send_frame)) else: label='no face detected' cv2.putText(self.new_frame,label,(10,30),cv2.FONT_HERSHEY_SIMPLEX,1.0,(0,255,0),2) cv2.imshow('Detection',self.new_frame) cv2.waitKey(1)","title":"node"},{"location":"Mask_Detector/#calling-the-node","text":"def main(args=None): rclpy.init(args=args) image_subscriber = MaskDetector() rclpy.spin(image_subscriber) image_subscriber.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()","title":"calling the node"},{"location":"Neural_Network_Model/","text":"Introduction The main objective of this project is to detect the absence or presence of facemask using the OAK-D camera feed of turtlebot by deploying Neural Networks. If the turtlebot camera detects a face without mask, it'll move towards the detected face. Neural Network for Face Mask Detection Model Requirements # The Neural Network for Face detection. # Code import pandas as pd import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense from matplotlib import pyplot as plt import numpy as np Training data #Step 1 Data train_df = pd.read_csv(r'D:\\Spring 23\\EGR 598\\project\\Data6.csv') train_df['label'] = train_df['label'].astype(str) train_datagen = ImageDataGenerator(rescale=1./255) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_dataframe( dataframe=train_df, directory=r'D:\\Spring 23\\EGR 598\\project\\train_images\\train_data6', x_col='filename', y_col='label', target_size=(300, 300), batch_size=32, class_mode='sparse', shuffle=True) Model Building # Step 2: Build and compile the model model = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu'), MaxPooling2D((2, 2)), # till here better accuracy Flatten(), Dense(64, activation='relu'), Dense(3, activation='softmax') ]) history =model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) acc_list = [] Training the Model # Step 3: Train the model history =model.fit(train_generator, epochs=5) for acc in history.history['accuracy']: acc_list.append(acc) weights = model.get_weights() Saving the Model Weights and Model # Step 4: saving model model.save_weights('my_model_weights.h5') model.save('mode_for_inference.h5')","title":"Neural Network Model"},{"location":"Neural_Network_Model/#introduction","text":"The main objective of this project is to detect the absence or presence of facemask using the OAK-D camera feed of turtlebot by deploying Neural Networks. If the turtlebot camera detects a face without mask, it'll move towards the detected face.","title":"Introduction"},{"location":"Neural_Network_Model/#neural-network-for-face-mask-detection-model","text":"","title":"Neural Network for Face Mask Detection Model"},{"location":"Neural_Network_Model/#requirements","text":"# The Neural Network for Face detection. # Code import pandas as pd import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense from matplotlib import pyplot as plt import numpy as np","title":"Requirements"},{"location":"Neural_Network_Model/#training-data","text":"#Step 1 Data train_df = pd.read_csv(r'D:\\Spring 23\\EGR 598\\project\\Data6.csv') train_df['label'] = train_df['label'].astype(str) train_datagen = ImageDataGenerator(rescale=1./255) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_dataframe( dataframe=train_df, directory=r'D:\\Spring 23\\EGR 598\\project\\train_images\\train_data6', x_col='filename', y_col='label', target_size=(300, 300), batch_size=32, class_mode='sparse', shuffle=True)","title":"Training data"},{"location":"Neural_Network_Model/#model-building","text":"# Step 2: Build and compile the model model = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu'), MaxPooling2D((2, 2)), # till here better accuracy Flatten(), Dense(64, activation='relu'), Dense(3, activation='softmax') ]) history =model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) acc_list = []","title":"Model Building"},{"location":"Neural_Network_Model/#training-the-model","text":"# Step 3: Train the model history =model.fit(train_generator, epochs=5) for acc in history.history['accuracy']: acc_list.append(acc) weights = model.get_weights()","title":"Training the Model"},{"location":"Neural_Network_Model/#saving-the-model-weights-and-model","text":"# Step 4: saving model model.save_weights('my_model_weights.h5') model.save('mode_for_inference.h5')","title":"Saving the Model Weights and Model"},{"location":"Velocity_subscriber/","text":"Introduction Real time ROS node for velocity subscriber Requirements # velocity Subscriber. # Code import rclpy from rclpy.node import Node from sensor_msgs.msg import Image from cv_bridge import CvBridge import cv2 from geometry_msgs.msg import Twist from mtcnn import MTCNN node #Step 1 Data class FaceTrackingNode(Node): def __init__(self): super().__init__('face_tracking') self.bridge = CvBridge() self.subscription = self.create_subscription(Image,'/go/image',self.image_callback,10) self.publisher = self.create_publisher(Twist, 'cmd_vel', 10) self.detector = MTCNN() def image_callback(self, msg): cv_image = self.bridge.imgmsg_to_cv2(msg) results = self.detector.detect_faces(cv_image) if results: bounding_box = results[0]['box'] x, y, w, h = bounding_box center_x = x + w/2 center_y = y + h/2 height, width, channels = cv_image.shape error_x = center_x - width/2 error_y = center_y - height/2 twist_msg = Twist() twist_msg.linear.x = 0.05 # set linear speed twist_msg.angular.z = -0.001 * error_x # set angular speed based on error self.publisher.publish(twist_msg) calling the node def main(args=None): rclpy.init(args=args) node = FaceTrackingNode() rclpy.spin(node) node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()","title":"Velocity Subscriber"},{"location":"Velocity_subscriber/#introduction","text":"","title":"Introduction"},{"location":"Velocity_subscriber/#real-time-ros-node-for-velocity-subscriber","text":"","title":"Real time ROS node for velocity subscriber"},{"location":"Velocity_subscriber/#requirements","text":"# velocity Subscriber. # Code import rclpy from rclpy.node import Node from sensor_msgs.msg import Image from cv_bridge import CvBridge import cv2 from geometry_msgs.msg import Twist from mtcnn import MTCNN","title":"Requirements"},{"location":"Velocity_subscriber/#node","text":"#Step 1 Data class FaceTrackingNode(Node): def __init__(self): super().__init__('face_tracking') self.bridge = CvBridge() self.subscription = self.create_subscription(Image,'/go/image',self.image_callback,10) self.publisher = self.create_publisher(Twist, 'cmd_vel', 10) self.detector = MTCNN() def image_callback(self, msg): cv_image = self.bridge.imgmsg_to_cv2(msg) results = self.detector.detect_faces(cv_image) if results: bounding_box = results[0]['box'] x, y, w, h = bounding_box center_x = x + w/2 center_y = y + h/2 height, width, channels = cv_image.shape error_x = center_x - width/2 error_y = center_y - height/2 twist_msg = Twist() twist_msg.linear.x = 0.05 # set linear speed twist_msg.angular.z = -0.001 * error_x # set angular speed based on error self.publisher.publish(twist_msg)","title":"node"},{"location":"Velocity_subscriber/#calling-the-node","text":"def main(args=None): rclpy.init(args=args) node = FaceTrackingNode() rclpy.spin(node) node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()","title":"calling the node"}]}