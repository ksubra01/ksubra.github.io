<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="None" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>Turtlebot based Face Mask Detection</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Home";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Turtlebot based Face Mask Detection
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href=".">Home</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#a-updatesrefreshes">a. Execution</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#b-project-process">b. Project Process</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#c-work-flow">c. Work Flow</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#d-ros-network">d. ROS Network</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#e-tradeoffs">e. Tradeoffs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#f-videos-of-working-model">f. Videos of Working Model</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#g-goals-process-and-results-s">g. Goals, Process and Results</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="Listing/">Code and Dependencies</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="Neural_Network_Model/">Neural Network Model</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="Mask_Detector/">Mask Detector</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="Face_Bounding_box/">Face Bounding Box</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="Velocity_subscriber/">Velocity Subscriber</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Project Face Mask Detection</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" alt="Docs"></a> &raquo;</li>
      <li>Home</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/ksubra01/ksubra.github.io/blob/main/index.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="introduction">Introduction</h1>
<p>The main objective of this project is to detect the absence or presence of facemask using the OAK-D camera feed of turtlebot by deploying Neural Networks.
If the turtlebot camera detects a face without mask, it displays the output image as 'without mask' and move towards the detected face.</p>
<p>Training data and Neural network for mask detection is custom made, MTCNN (inbuilt algorithm) is being used for face coordinates.</p>
<p>An insight of Project Goals and procedure that we have followed and results obtained are briefly discussed 
<a href="#g">here</a></p>

<h2 id="a-updatesrefreshes">a. Updates/Refreshes</h2>
<ol>
<li>We made our own Neural Network for the feature generation by taking training data of images from OAK-D camera.</li>
<li>From the custom NN model, We deployed model to ROS node which does face mask recognition in real time by subscribing to real time data current_frame.</li>
<li>We used inbuilt software called MTCNN face detection algorithm for creation of the bounding box for faces which is also subscribing to real time data current_frame.</li>
<li>We published a custom topic /go/image from face mask recognition ROS node to MTCNN_vel_pub(fcae tracking) node which is subscriber to /go/image and publishes /cmd/vel.</li>
<li>If the MTCNN_vel_det(face tracking) is being sent face without mask, it publishes velocity to move towards that face by using Face bounding box coordinates.</li>
</ol>
<p>We tried mapping classroom environment, but due to constraints we sticked to main scope of project which is NN deployment. We aim to improve by performing dynamic obstacle avoidance (SLAM).</p>
<h2 id="b-project-process">b. Project Process</h2>
<p>Although we were able to deploy custom model to ROS node in the previous week, Due to accuracy and some other constraints we revised the schedule from the last week and performed required tasks such as data collection and real time deployment with iterations.</p>
<p>Following is the <strong>Process chart</strong>:</p>
<p><img alt="Home" src="img/gantt.png" /></p>
<p>The <strong>complete work flow of our project</strong> as follows:</p>
<p><img alt="Home" src="img/Screenshot.png" /></p>
<h2 id="c-work-flow">c. Work Flow</h2>
<ul>
<li>
<p>Initially we collected dataset consisting of huge variations i.e 
Face with mask and Without Mask at different distances from OAK-D camera at different backgrounds with multiple gestures being performed.</p>
</li>
<li>
<p><strong>Few Instances of preliminary dataset</strong></p>
</li>
<li><strong>With Face Mask</strong>  -  If the frame has a face with mask. - Class 0</li>
</ul>
<p><img alt="H" src="img/Picture1.jpg" /><img alt="j" src="img/Picture2.jpg" /><img alt="I" src="img/Picture3.jpg" /><img alt="II" src="img/Picture4.jpg" /><img alt="I" src="img/Picture5.jpg" /><img alt="I" src="img/Picture8.jpg" /></p>
<ul>
<li><strong>Without Face Mask</strong> - If the frame has a face without mask and some ambiguity cases of half frames and fully blurred faces. - Class 1</li>
</ul>
<p><img alt="H" src="img/Picture12.jpg" /><img alt="j" src="img/Picture13.jpg" /><img alt="I" src="img/Picture14.jpg" /><img alt="II" src="img/Picture15.jpg" /><img alt="I" src="img/Picture17.jpg" /><img alt="I" src="img/Picture18.jpg" /></p>
<ul>
<li><strong>No recognizable face in Frame</strong> - If the frame has no recognizable face and some distant images which are very unclear. - Class 2</li>
</ul>
<p><img alt="H" src="img/Picture31.jpg" /><img alt="j" src="img/Picture30.jpg" /><img alt="I" src="img/Picture24.jpg" /><img alt="II" src="img/Picture25.jpg" /><img alt="I" src="img/Picture27.jpg" /><img alt="I" src="img/Picture29.jpg" /></p>
<p><strong>Custom Neural Network for Face mask Detection:</strong></p>
<p><img alt="H" src="img/NN.png" /></p>
<p>Then with the custom Neural Network we devloped, we generated feature maps which shows as following:</p>
<p><img src="./img/Picture34.png" alt="Image 1" width="300" height="300">
<img src="./img/Picture33.png" alt="Image 1" width="300" height="300">
<img src="./img/Picture35.png" alt="Image 1" width="300" height="300">
<img src="./img/Picture32.png" alt="Image 1" width="300" height="300"></p>
<p>Even in this zoomed in images, the amount of pixels covering pixels are very few and very difficult to classify the images based on mask from the limited data with high number of variations.</p>
<ul>
<li>Although training Accuracy is good by optimizing the algorithm and increasing number of epochs, <strong>testing accuracy is not good with only 65%</strong>. If we perform real time detection with this, it would be highly inaccurate.</li>
</ul>
<p><img src="./img/Error.png" alt="Image 1" width="500" height="500"></p>
<p><strong>Counter Measures</strong></p>
<ul>
<li>
<p>We updated the Training data with Face with and without mask at limited distance at proper light source with few gestures.</p>
</li>
<li>
<p><strong>Few Instances of preliminary dataset</strong></p>
</li>
<li><strong>With Face Mask</strong>  -  If the frame has face with mask</li>
</ul>
<p><img src="./img/wm1.jpg" alt="Image 1" width="200" height="200">
<img src="./img/wm2.jpg" alt="Image 1" width="200" height="200">
<img src="./img/wm3.jpg" alt="Image 1" width="200" height="200"></p>
<ul>
<li><strong>Without Face Mask</strong> - If the frame has consisting of face without mask and some ambiguity cases of half frames and fully blurred faces.</li>
</ul>
<p><img src="./img/w1.jpg" alt="Image 1" width="200" height="200">
<img src="./img/w2.jpg" alt="Image 1" width="200" height="200">
<img src="./img/w3.jpg" alt="Image 1" width="200" height="200"></p>
<ul>
<li><strong>No recognizable face in Frame</strong> - If the frame has no recognizable face and some distant images which are very unclear.</li>
</ul>
<p><img src="./img/n1.jpg" alt="Image 1" width="200" height="200">
<img src="./img/n2.jpg" alt="Image 1" width="200" height="200">
<img src="./img/n3.jpg" alt="Image 1" width="200" height="200"></p>
<ul>
<li>
<p>we split the data into Training and Testing in 90% and 10% and Training accuracy as follows:</p>
</li>
<li>
<p>Training data Classification:
<img src="./img/Training_data.png" alt="Image 1" width="500" height="500"></p>
</li>
<li>
<p>Testing data Classification:
<img src="./img/Testing_data.png" alt="Image 1" width="500" height="500"></p>
</li>
<li>
<p>After Training on data, we got adequate accuracy in Training along with a <strong>good amount of testing accuracy of 89%</strong></p>
</li>
<li>
<p>Training Accuracy:</p>
</li>
</ul>
<p><img src="./img/Final_accuracy.png" alt="Image 1" width="500" height="500"></p>
<ul>
<li>
<p>With 89% testing accuracy, we tested on few absolute new images as well before deploying to ROS node for real time detection.
It is predicting good, doesn't take much time as well. </p>
</li>
<li>
<p>We transferred the model_infernce to ROS node using tensorflow.</p>
</li>
</ul>
<h2 id="d-ros-network">d. ROS Network</h2>
<ul>
<li>
<p>With model being imported, we created total of 3 nodes and 1 custom topic to complete actions.</p>
</li>
<li>
<p><strong>Mask Detector node</strong> is a subscriber to /color/image and publishes custom topic /go/image, displays output windows of current frame and detection frame.</p>
</li>
<li><strong>face Detector node</strong> is a subscriber to /color/image and displays output window of bounding box.</li>
<li>
<p><strong>face tracking</strong> node is a subscriber to /go/image and publishes /cmd/vel.</p>
</li>
<li>
<p><strong>Complete ROS architecture</strong></p>
</li>
</ul>
<p><img alt="H" src="img/rosgraph_overall.png" />
      rqt_gul_py for the generation of ROS graph is also shown in the graph.</p>
<h2 id="e-tradeoffs">e. Tradeoffs</h2>
<p><strong>Main Challenges</strong>  </p>
<ol>
<li>
<p><strong>Training time</strong></p>
<ul>
<li>As the Neural Network model is heavy, we trained set of images in GPU and obtained the model.
  Then transferred the model weights to ROS node to take one frame at a time.This has no lag and almost accurate.</li>
</ul>
</li>
<li>
<p><strong>Movement of Turtlebot</strong></p>
<ul>
<li>For the Face detection and creating a bounding box, it takes more time. As a result, output window with bounding box has a lag of few seconds.
  This in turn carries the lag in moving the turtlebot towards the bounding box containing face without the mask.</li>
</ul>
</li>
<li>
<p><strong>Detection vs Movement</strong></p>
<ul>
<li>Although Face mask detection is done in real time, turtlebot movement is seen after a few moments of detection.</li>
</ul>
</li>
</ol>
<p><strong>Tradeoffs due to constraints</strong></p>
<ol>
<li>
<p><strong>Fitting Training data</strong></p>
<ul>
<li>We used Adam algorithm to save the training time.</li>
<li>We chose number of epochs to 5 to not overfit on training data by considering bias and variance trade off.</li>
<li>If we furthur increase the number of epochs, we may lose the testing accuracy.</li>
</ul>
</li>
<li>
<p><strong>Non Generalized Detection</strong></p>
<ul>
<li>As the dataset is limited, detection using OAK-D camera at different environment with different person is not feasible.</li>
<li>To detect, proper lighting and limited distance to the camera are required.</li>
<li>With limited training data, we are able to test at few places with absolute new faces successfully.</li>
</ul>
</li>
<li>
<p><strong>Bounding box trade off</strong></p>
<ul>
<li>We used face detection algorithm along with velocity subscriber not with face mask detector to increase the speed of detection.</li>
<li>It significantly increased the speed of detection by almost 2 to 3 sec at the cost of movement of bot.</li>
</ul>
</li>
</ol>
<p>In overall process, we gave stress to reduce type 2 error <strong>(False Negative)</strong>.<br />
1. As it is a face mask detection, a person without mask should be detected properly. <br />
2. If the bot has ambiguous case like image is blur or face is not complete in the frame, it is mostly detected as face without mask.<br />
3. If the face is mostly out of the frame, it is being detected as no face recognized.<br />
4. We trained the data accordingly to reduce the false negatives due to which there may be few errors during the transition of frames such as face entering or exiting the frame.  </p>
<h2 id="f-videos-of-working-model">f. Videos of Working Model</h2>
<p><strong>Video 1- TURTLEBOT VIEW OF TESTING</strong><br />
<strong>Real time video of turtlebot view detecting face without mask</strong>:<br />
Screen recording of mask is being pulled down and not covering the face. We have not trained this ambiguous cases earlier, still it is successfully detecting the classification  </p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/A22qfrodaQM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p><strong>Video 2- AMBIGUOUS CASE TESTING WITH NO TRAINING</strong><br />
<strong>Real time video of turtlebot detecting face without mask</strong>: <br />
* In this video, turtlebot movement of simultaneous ROS nodes in video1.  </p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HcZVg-6S_Ns" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p><strong>Video 3 - ABSOLUTE UNKNOWN FACE TESTING</strong><br />
<strong>RealTime detection of Presence or Ansence of mask with absolute zero training of Person's face</strong><br />
*  In this video, Face present in the frame is not at all used in training data. It is still successfully able to detect the classification accurately and moving towards the face without mask.  </p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pe2afb0SzxE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<h2 id="g-goals-process-and-results-s">g. Goals, Process and Results <a name="g"></a>s</h2>
<ul>
<li>An insight of Project Goals and procedure that we have followed and results obtained are briefly discussed in this:</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1o9Nh7-eACc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="Listing/" class="btn btn-neutral float-right" title="Code and Dependencies">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/ksubra01/ksubra.github.io" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
    
      <span><a href="Listing/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme_extra.js" defer></script>
    <script src="js/theme.js" defer></script>
      <script src="search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>

<!--
MkDocs version : 1.4.2
Build Date UTC : 2023-05-04 03:04:03.261729+00:00
-->
